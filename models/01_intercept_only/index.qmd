---
title: "Stan 101"
subtitle: "Intercept-only model"
format: html
---

```{r}
#| label: setup
#| include: false
source(here::here("scripts", "00_libraries.R"))
```

The intercept-only model will take the format `y ~ 1` in standard `lme4`-style syntax. 
We will assume the data generating process for `y` is a normal distribution, which takes the parameters $\mu$ and $\sigma$. 
The full model can be described in @eq-model: 

$$
\begin{aligned}
y_{i}  & \sim Normal(\mu, \sigma) & [Likelihood]\\
\mu    & \sim Normal(0, 1)        & [Prior] \\
\sigma & \sim Normal_{+}(0, 1)    & [Prior] \\
\end{aligned}
$$ {#eq-model}

First, we will simulate some data from the normal distribution with a mean of 0 and scale 1.

```{r}
#| label: data-gen
dat_df <- tibble(y = rnorm(1000))
dat_df |> summarize(mean = mean(y), sd = sd(y))
```

In `brms` we can fit this model like this: 

```{r}
#| label: brms-mod
mod_brms <- brm(
  formula = y ~ 1, 
  family = gaussian(), 
  prior = c(
    prior(normal(0, 1), class = Intercept), 
    prior(normal(0, 1), class = sigma)
  ), 
  data = dat_df, 
  cores = 4, 
  file = here("mods", "mod_int")
)
```

Now, we will write the same model in Stan. 
**Note**: You have to use the chunk option `output.var` to access the Stan code after the fact. 
In this case, we will assign the Stan code to the object `model`.


```{stan}
#| label: write-model
#| output.var: "model"
#| cache: true

functions {
// ... function declarations and definitions ...

}

data {
// ... declarations ...
  int<lower=0> n; // number of observations
  vector[n] y;    // criterion
  int prior_only; // Should the likelihood be ignored?
}

transformed data {
// ... declarations ... statements ...

}

parameters {
// ... declarations ...
  real Intercept;
  real<lower=0> sigma;
}

transformed parameters {
// ... declarations ... statements ...
  real lprior = 0;
  lprior += normal_lpdf(Intercept | 0, 1);
  lprior += normal_lpdf(sigma | 0, 1) - normal_lccdf(0 | 0, 1); // offset scale
}

model {
// ... declarations ... statements ...
  // Likelihood
  if (!prior_only) {
  // Initialize linear predictor term
  vector[n] mu = rep_vector(0.0, n);
  mu += Intercept;
  target += normal_lpdf(y | mu, sigma);
  }
  target += lprior;
}

generated quantities {
// ... declarations ... statements ...
  real b_Intercept = Intercept;
}
```

Now we can fit the model. 
We have to do a few things: 

1. We use `tidybayes::compose_data` to convert our data frame to a list format that plays nicely with Stan.
2. We use `rstan::sampling` to fit the object `model` from the previous chunk. 
3. We pass the data in the form of a list (created in (1)). 
4. We assign the output to the object `mod`. 

```{r}
#| label: fit-mod
#| cache: true
# Get list of data
dat_ls <- dat_df |> 
  compose_data(prior_only = as.integer(FALSE))

# Fit model
mod <- sampling(
  object = model, 
  data = dat_ls, 
  cores = 4
)

```

Now we can do all the normal stuff to the model object. 

```{r}
#| label: play
print(mod, pars = c("b_Intercept", "sigma"), probs = c(0.025, 0.975))

posterior <- as.data.frame(mod)

posterior |> 
  ggplot() + 
  aes(x = b_Intercept, y = sigma) + 
  geom_point(pch = 15, alpha = 0.3) + 
  geom_point(
    data = posterior |> summarize(b_Intercept = mean(b_Intercept), sigma = mean(sigma)), 
    pch = 21, color = "white", fill = "#cc0033", size = 5
  ) + 
  ds4ling::ds4ling_bw_theme()

posterior |> 
  pivot_longer(
    cols = c("b_Intercept", "sigma"), 
    names_to = "pars", 
    values_to = "estimate"
  ) |> 
  ggplot() + 
  aes(x = estimate, y = pars) + 
  stat_halfeye() + 
  ds4ling::ds4ling_bw_theme()
```



